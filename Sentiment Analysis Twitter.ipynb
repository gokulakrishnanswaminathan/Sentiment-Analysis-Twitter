{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tweepy\n",
    "import time\n",
    "from datetime import datetime\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "import string\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['username','tweet_date','retweet_count', 'tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping might not be perfect...working to fix the issue....\n",
    "consumer_key = ''\n",
    "consumer_secret = ''\n",
    "access_token = ''\n",
    "access_token_secret = ''\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)\n",
    "username = input('enter for username (ex.@POTUS) or hashtag (ex.#covid19):')\n",
    "i=0\n",
    "for _ in range(1):\n",
    "    dates = [datetime.datetime(2020, 5, 15, 0, 0, 0),datetime.datetime(2020, 5, 16, 0, 0, 0),datetime.datetime(2020, 5, 17, 0, 0, 0),]\n",
    "    for starting_time in dates:\n",
    "        for tweet in (tweepy.Cursor(api.search,q=username,count=100,\n",
    "                                   lang=\"en\",\n",
    "                                   since=str(starting_time),\n",
    "                                   until=str(starting_time+datetime.timedelta(days=1)),\n",
    "                                  tweet_mode='extended',\n",
    "                                  since_id=None).items(10000)):\n",
    "            try:\n",
    "                tweet_full_text = tweet.retweeted_status.full_text\n",
    "            except AttributeError:  # Not a Retweet\n",
    "                tweet_full_text = tweet.full_text\n",
    "            df = df.append({'username':tweet.user.screen_name,\n",
    "                    'tweet_date': tweet.created_at,\n",
    "                    'retweet_count': tweet.retweet_count,\n",
    "                    'tweet':tweet_full_text},ignore_index=True)\n",
    "\n",
    "            if (i+1)%1000==0:\n",
    "                print('%d tweets downloaded'%(i+1))\n",
    "            i+=1\n",
    "        time.sleep(15*60)\n",
    "to_csv_timestamp = datetime.today().strftime('%Y%m%d_%H%M%S')\n",
    "filename = to_csv_timestamp+'data.csv'\n",
    "df.to_csv(filename,encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('filename.csv',engine='python')\n",
    "username = '#covid19'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=df.columns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dropna(subset = ['tweet'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop_duplicates(keep=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = []\n",
    "for i in range(0, len(data)):\n",
    "    ith_date_str = str(data.iloc[i, :]['tweet_date'])\n",
    "    ith_match = re.search(r'\\d{4}-\\d{2}-\\d{2}', ith_date_str)\n",
    "    ith_date = datetime.strptime(ith_match.group(), '%Y-%m-%d').date()\n",
    "    date.append(ith_date)\n",
    "data['dates'] = date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_pattern = re.compile(\"[\"\n",
    "         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "         u\"\\U00002702-\\U000027B0\"\n",
    "         u\"\\U000024C2-\\U0001F251\"\n",
    "         \"]+\", flags=re.UNICODE)\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    if pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "def clean_text(x):\n",
    "    x = re.sub('@[^\\s]+', '',x)\n",
    "    x = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', '', x)\n",
    "    x = re.sub(r'#([^\\s]+)', '', x)\n",
    "    x = re.sub(r'^(?i)[RT]+','',x)\n",
    "    x = emoji_pattern.sub(r'', x)\n",
    "    x = x.lower()\n",
    "    x = [i.strip(string.punctuation) for i in x.split(\" \")]\n",
    "    stopword = stopwords.words('english')\n",
    "    x = [i for i in x if i not in stopword]\n",
    "    x = [i for i in x if len(i)>1]\n",
    "    pos_tags = pos_tag(x)\n",
    "    x = [WordNetLemmatizer().lemmatize(i[0],get_wordnet_pos(i[1])) for i in pos_tags]\n",
    "    x = \" \".join(x)\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet_c'] = data.tweet.apply(lambda j: clean_text(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "data['sentiment'] = data.tweet_c.apply(lambda x: sia.polarity_scores(x))\n",
    "data = pd.concat([data.drop(['sentiment'], axis = 1), data['sentiment'].apply(pd.Series)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentclass_list = []\n",
    "for i in range(0, len(data)):\n",
    "    curr_compound = data.compound.iloc[i]\n",
    "    if (curr_compound <= 1.0 and curr_compound >= 0.55):\n",
    "        sentimentclass_list.append(5)\n",
    "    elif (curr_compound < 0.55 and curr_compound >= 0.10):\n",
    "        sentimentclass_list.append(4)\n",
    "    elif (curr_compound < 0.10 and curr_compound > -0.10):\n",
    "        sentimentclass_list.append(3)\n",
    "    elif (curr_compound <= -0.10 and curr_compound > -0.55):\n",
    "        sentimentclass_list.append(2)\n",
    "    elif (curr_compound <= -0.55 and curr_compound >= -1.00):\n",
    "        sentimentclass_list.append(1)\n",
    "data['sentiment_class'] = sentimentclass_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "very_pos_text=\"\"\n",
    "very_neg_text=\"\"\n",
    "neut_text=\"\"\n",
    "pos_text=\"\"\n",
    "neg_text=\"\"\n",
    "for i in range(len(data)):\n",
    "    if (data.iloc[i,:]['sentiment_class'])==5:\n",
    "        very_pos_text+=data.iloc[i,:]['tweet_c']\n",
    "    elif (data.iloc[i,:]['sentiment_class'])==4:\n",
    "        pos_text+=data.iloc[i,:]['tweet_c']\n",
    "    elif (data.iloc[i,:]['sentiment_class'])==2:\n",
    "        neg_text+=data.iloc[i,:]['tweet_c']\n",
    "    elif (data.iloc[i,:]['sentiment_class'])==1:\n",
    "        very_neg_text+=data.iloc[i,:]['tweet_c']\n",
    "    else:\n",
    "        neut_text+=data.iloc[i,:]['tweet_c']\n",
    "list_text = {'Positive':pos_text,\n",
    "             'Negative':neg_text,\n",
    "             'Netural':neut_text,\n",
    "            'Very Positive':very_pos_text,\n",
    "            'Very Negative':very_neg_text}\n",
    "for text in list_text:\n",
    "    if len(list_text[text])>0:\n",
    "        wordcloud = WordCloud(\n",
    "        background_color = 'black',\n",
    "        max_words = 100,\n",
    "        max_font_size = 40, \n",
    "        scale = 3,\n",
    "        random_state = 42\n",
    "    ).generate(list_text[text])\n",
    "        plt.figure(1, figsize = (20, 15))\n",
    "        plt.axis('off')\n",
    "        plt.title(text, fontsize = 20)\n",
    "        plt.imshow(wordcloud)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if username[0]=='#':\n",
    "    data.tweet = data.tweet.str.replace(username,'',regex=True,flags=re.IGNORECASE)\n",
    "else:\n",
    "    data.tweet = data.tweet.str.replace(username,'',regex=True,flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtaglist_d,mentionslist_d = [],[]\n",
    "hashtaglist_d.append(data.tweet.apply(lambda x: list(set(re.findall(r'#[A-Z0-9a-z:]+',x)))))\n",
    "mentionslist_d.append(data.tweet.apply(lambda x: list(set(re.findall(r'@[A-Z0-9a-z:]+',x)))))\n",
    "hashtaglist,mentionslist = [],[]\n",
    "for i in hashtaglist_d[0]:\n",
    "    for j in i:\n",
    "        hashtaglist.append(j)\n",
    "for i in mentionslist_d[0]:\n",
    "    for j in i:\n",
    "        mentionslist.append(j)\n",
    "df_hashtag = pd.DataFrame({'hashtags':hashtaglist})\n",
    "df_mentions_on_user = pd.DataFrame({'Mentions':mentionslist})\n",
    "top_hashtags = df_hashtag.groupby(['hashtags']).size().reset_index(name = 'counts').sort_values(by = 'counts', ascending = False).head(15)\n",
    "top_mentions = df_mentions_on_user.groupby(['Mentions']).size().reset_index(name = 'counts').sort_values(by = 'counts', ascending = False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.retweet_count = pd.to_numeric(data.retweet_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_tweets_df = data.sort_values(by = ['retweet_count'], ascending = False)\n",
    "top_tweets_df.drop_duplicates(subset='tweet',keep='first',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_hashtags = pd.DataFrame(columns = ['hashtags', 'count', 'date', 'dayofnov'])\n",
    "mentionseries_hashtags = pd.DataFrame(columns = ['mentions', 'count', 'date', 'dayofnov'])\n",
    "unique_date = data.dates.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizing(main_df, timeseries_df,mentionseries_df, N, T, unique_dates):\n",
    "    counter = 1\n",
    "    for ith_date in reversed(unique_dates):\n",
    "        if counter <= T:\n",
    "            ith_date_df = main_df[main_df['dates'] == ith_date]\n",
    "            ith_hashtag_list = []\n",
    "            ith_mention_list = []\n",
    "            hashtaglist_d,mentionslist_d = [],[]\n",
    "            hashtaglist_d.append(ith_date_df.tweet.apply(lambda x: list(set(re.findall(r'#[A-Z0-9a-z:]+',x)))))\n",
    "            mentionslist_d.append(ith_date_df.tweet.apply(lambda x: list(set(re.findall(r'@[A-Z0-9a-z:]+',x)))))\n",
    "            for i in hashtaglist_d[0]:\n",
    "                for j in i:\n",
    "                    ith_hashtag_list.append(j)\n",
    "            for i in mentionslist_d[0]:\n",
    "                for j in i:\n",
    "                    ith_mention_list.append(j)\n",
    "            ith_df_hashtag = pd.DataFrame({'hashtags': ith_hashtag_list})\n",
    "            ith_df_mention = pd.DataFrame({'mentions': ith_mention_list})\n",
    "            ith_top_hashtags = ith_df_hashtag.groupby(['hashtags']).size().reset_index(name = 'count').sort_values(by = 'count', ascending = False).head(N)\n",
    "            ith_top_mentions = ith_df_mention.groupby(['mentions']).size().reset_index(name = 'count').sort_values(by = 'count', ascending = False).head(N)\n",
    "            ith_top_hashtags['date'] = ith_date\n",
    "            ith_top_hashtags['dayofnov'] = ith_date.day\n",
    "            ith_top_mentions['date'] = ith_date\n",
    "            ith_top_mentions['dayofnov'] = ith_date.day\n",
    "            timeseries_df = pd.concat([timeseries_df, ith_top_hashtags], axis = 0)\n",
    "            mentionseries_df = pd.concat([mentionseries_df, ith_top_mentions], axis = 0)\n",
    "            counter += 1\n",
    "        else: \n",
    "            break\n",
    "    timeseries_df.reset_index(inplace = True, drop = True)\n",
    "    mentionseries_df.reset_index(inplace = True, drop = True)\n",
    "    return timeseries_df,mentionseries_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_hashtags,mentionseries_hashtags = visualizing(main_df = data,\n",
    "                       timeseries_df = timeseries_hashtags,\n",
    "                        mentionseries_df = mentionseries_hashtags,\n",
    "                       N = 15,\n",
    "                       T = 4,\n",
    "                       unique_dates = unique_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows = 6, cols = 12,\n",
    "    specs = [\n",
    "        [None,None,None,{\"type\": \"bar\", \"colspan\":4},None,None,{\"type\": \"bar\", \"colspan\":4},None,None,None,None,None],\n",
    "        [{'type':'pie','rowspan':3,'colspan':3},None,None,{\"type\": \"bar\", \"colspan\":4},None,None,None,None,{\"type\": \"bar\", \"colspan\":4},None,None,None],\n",
    "        [None,None,None,{\"type\": \"bar\", \"colspan\":4},None,None,None,None,{\"type\": \"bar\", \"colspan\":4},None,None,None],\n",
    "        [None,None,None,None,None,None,None,None,None,None,None,None],\n",
    "        [{'type':'table','colspan':3},None,None,None,{'type':'table','colspan':3},None,None,None,{'type':'table','colspan':3},None,None,None],\n",
    "         [None,None,{'type':'table','colspan':3},None,None,None,{'type':'table','colspan':3},None,None,None,None,None]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "fig.add_trace(go.Bar(\n",
    "                x = top_hashtags.hashtags.tolist(),\n",
    "                y = top_hashtags.counts.tolist(),\n",
    "                name = 'Top hashtags',\n",
    "                showlegend = True),\n",
    "             row = 2 ,col = 4 )\n",
    "fig.add_trace(go.Bar(\n",
    "                x = top_mentions.Mentions.tolist(),\n",
    "                y = top_mentions.counts.tolist(),\n",
    "                name = 'Top Mentions',\n",
    "                showlegend = True),\n",
    "             row = 3 ,col = 4 )\n",
    "for i in np.unique(timeseries_hashtags.date):\n",
    "    dum = timeseries_hashtags[timeseries_hashtags.date==i]\n",
    "    fig.add_trace(go.Bar(\n",
    "                x = dum.hashtags.tolist(),\n",
    "                y = dum['count'].tolist(),\n",
    "                name = 'Top hashtags in'+'\\t'+str(i),\n",
    "                showlegend = True),\n",
    "             row = 2 ,col = 9)\n",
    "for i in np.unique(mentionseries_hashtags.date):\n",
    "    dum = mentionseries_hashtags[mentionseries_hashtags.date==i]\n",
    "    fig.add_trace(go.Bar(\n",
    "                x = dum.mentions.tolist(),\n",
    "                y = dum['count'].tolist(),\n",
    "                name = 'Top Mentions in'+'\\t'+str(i),\n",
    "                showlegend = True),\n",
    "             row = 3 ,col = 9)\n",
    "fig.add_trace(go.Pie(labels=data.sentiment_class.replace({5:'Very Positive',4:'Positive',3:'Neutral',2:'Negative',1:'Very Negative'}).unique().tolist(),\n",
    "       values = data.sentiment_class.replace({5:'Very Positive',4:'Positive',3:'Neutral',2:'Negative',1:'Very Negative'}).value_counts().tolist(),\n",
    "                        textposition = \"inside\",showlegend=False,\n",
    "                         texttemplate='%{label}: %{value:} <br>(%{percent})',\n",
    "                        hoverinfo='none',\n",
    "                        hole =0.3,\n",
    "                         insidetextorientation='radial',\n",
    "                    title={'text':'Sentiment Analysis','position':'middle center'}),row=2,col = 1)\n",
    "fig.add_trace( go.Table(\n",
    "    header = dict(values = list(['Positive Tweet']),\n",
    "                 fill_color = 'blue',\n",
    "                 align='center'),\n",
    "    cells = dict(values=[data[data.sentiment_class==4].tweet.iloc[np.random.randint(0,len(data[data.sentiment_class==4]),5)]],\n",
    "                align = 'left')),row = 5,col = 1\n",
    ")\n",
    "fig.add_trace( go.Table(\n",
    "    header = dict(values = list(['Very Positive tweet']),\n",
    "                 fill_color = 'orange',\n",
    "                 align='center'),\n",
    "    cells = dict(values=[data[data.sentiment_class==5].tweet.iloc[np.random.randint(0,len(data[data.sentiment_class==5]),5)]],\n",
    "                align = 'left')),row = 6,col = 3\n",
    ")\n",
    "fig.add_trace( go.Table(\n",
    "    header = dict(values = list(['Neutral tweet']),\n",
    "                 fill_color = 'purple',\n",
    "                 align='center'),\n",
    "    cells = dict(values=[data[data.sentiment_class==3].tweet.iloc[np.random.randint(0,len(data[data.sentiment_class==3]),5)]],\n",
    "                align = 'left')),row = 5,col = 9\n",
    ")\n",
    "fig.add_trace( go.Table(\n",
    "    header = dict(values = list(['Negative tweet']),\n",
    "                 fill_color = 'green',\n",
    "                 align='center'),\n",
    "    cells = dict(values=[data[data.sentiment_class==2].tweet.iloc[np.random.randint(0,len(data[data.sentiment_class==2]),5)]],\n",
    "                align = 'left')),row = 5,col = 5\n",
    ")\n",
    "fig.add_trace( go.Table(\n",
    "    header = dict(values = list(['Very Negative tweet']),\n",
    "                 fill_color = 'red',\n",
    "                 align='center'),\n",
    "    cells = dict(values=[data[data.sentiment_class==1].tweet.iloc[np.random.randint(0,len(data[data.sentiment_class==1]),5)]],\n",
    "                align = 'left')),row = 6,col = 7\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    template=\"plotly_dark\",\n",
    "    title = {'text':\"Twitter Analysis for \"+username,'x':0.5,'y':0.9,'xanchor':'center'},\n",
    "    titlefont = {'size':40},\n",
    "    \n",
    "    showlegend=True,\n",
    "    legend_orientation=\"v\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.write_html('Twitter.html', auto_open=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit",
   "language": "python",
   "name": "python38164bit085c9d3b163b4d569833a13d00a1689b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
